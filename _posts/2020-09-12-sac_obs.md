---
title:  "Curious Observation in SAC Implementation"
permalink: /posts/2020/09/12/sac_obs
tags:
  - reinforcement-learning
  - pytorch
categories: 
  - implementation
---

While working on my entry for the [MineRL 2020](https://www.aicrowd.com/challenges/neurips-2020-minerl-competition#f.a.q) competition, I had to implement the soft-actor critic (SAC) algorithm from scratch. However, while coding the calculation of the log probabilities of the actions, I came across a tiny obstacle.

Let me provide a little context here: the policy network outputs the **mean and variance of a Gaussian distribution**, from which we can get our actions via the reparametrization trick:

$$ \mathbf{a}_t = \mu_\phi(\mathbf{s}_t) + \sigma^2_\phi(\mathbf{s}_t) \cdot \epsilon  \quad \text{where}, \epsilon \sim \mathcal{N}(0,1)$$

But we need to have our actions bounded, and so we take a $ \mathrm{tanh} $ over the sampled actions, $ \tilde{\mathbf{a}}_t = \mathrm{tanh}(\mathbf{a}_t) $. This "squashing" requires for an appropriate correction when calculating $ \log \pi (\mathbf{a}_t \| \mathbf{s}_t) $, which the authors detail as:

$$ \begin{equation}
\log \pi(\tilde{\mathbf{a_t}} \mid \mathbf{s_t})=\log \mathcal{N}_\phi(\mathbf{a}_t \mid \mathbf{s_t}, \epsilon)-\sum_{i=1}^{D} \log \left(1-\tanh ^{2}\left(a_{i}\right)\right)
\label{eq1} \end{equation} $$

where, $ \mathcal{N}_\phi $ represents the reparametrized Gaussian distribution. 

In code, you can achieve this simply by:
```ruby
dist = Normal(mean, std)
# sample actions from reparametrized distribution
gauss_actions = dist.rsample()
# squashed actions
actions = torch.tanh(gauss_actions)
# log probabilities
log_prob = dist.log_prob(gauss_actions) - 
        torch.sum(torch.log(1 - actions ** 2), dim=1)
```
This is what I wrote the first time I coded up the algorithm. However if you try and run this, you will notice that `dist.log_prob()` returns a vector instead of a scalar, which is where the problem began for me.

After comparing this with other implementaitons, I noticed a key operation I had missed. Take for example the [stable-baselines](https://github.com/DLR-RM/stable-baselines3/blob/583d4b8e4105977d0aebf8b467084af9ac57bebf/stable_baselines3/common/distributions.py#L163-L164) implementation which boils down to the following:

```ruby
log_prob = dist.log_prob(gauss_actions).sum(dim=1) - 
        torch.sum(torch.log(1 - actions ** 2), dim=1)
```
